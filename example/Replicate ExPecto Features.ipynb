{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4'\n",
    "\n",
    "#Purpose of this notebook is to replicate ExPecto chromatin features\n",
    "\n",
    "#start by reading in the .npy features\n",
    "npy_features_file = \"/srv/scratch/avanti/ExPecto/resources/Xreducedall.2002.npy\"\n",
    "expecto_features = np.load(npy_features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#now read in the gene file\n",
    "gene_file = \"/srv/scratch/avanti/ExPecto/resources/geneanno.csv\"\n",
    "gene_chrom_tss_strand = []\n",
    "for i,line in enumerate(open(gene_file)):\n",
    "    gene_id,symbol,chrom,strand,TSS,CAGE_TSS,gene_type = line.rstrip().split(\",\")\n",
    "    if (i > 0):\n",
    "        gene_chrom_tss_strand.append((gene_id, chrom, int(CAGE_TSS), (1 if strand==\"+\" else -1)) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (0) -> (1) -> (2) -> output]\n",
       "  (0): nn.Sequential {\n",
       "    [input -> (0) -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> (13) -> (14) -> (15) -> output]\n",
       "    (0): nn.SpatialConvolution(4 -> 320, 1x8)\n",
       "    (1): nn.ReLU\n",
       "    (2): nn.SpatialConvolution(320 -> 320, 1x8)\n",
       "    (3): nn.ReLU\n",
       "    (4): nn.Dropout(0.2000)\n",
       "    (5): nn.SpatialMaxPooling(1x4, 1, 4)\n",
       "    (6): nn.SpatialConvolution(320 -> 480, 1x8)\n",
       "    (7): nn.ReLU\n",
       "    (8): nn.SpatialConvolution(480 -> 480, 1x8)\n",
       "    (9): nn.ReLU\n",
       "    (10): nn.Dropout(0.2000)\n",
       "    (11): nn.SpatialMaxPooling(1x4, 1, 4)\n",
       "    (12): nn.SpatialConvolution(480 -> 640, 1x8)\n",
       "    (13): nn.ReLU\n",
       "    (14): nn.SpatialConvolution(640 -> 640, 1x8)\n",
       "    (15): nn.ReLU\n",
       "  }\n",
       "  (1): nn.Sequential {\n",
       "    [input -> (0) -> (1) -> (2) -> (3) -> (4) -> output]\n",
       "    (0): nn.Dropout(0.5000)\n",
       "    (1): nn.Reshape(67840)\n",
       "    (2): nn.Linear(67840 -> 2003)\n",
       "    (3): nn.ReLU\n",
       "    (4): nn.Linear(2003 -> 2002)\n",
       "  }\n",
       "  (2): nn.Sigmoid\n",
       "}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now load the pytorch model\n",
    "import torch\n",
    "from torch.utils.serialization import load_lua\n",
    "\n",
    "model = load_lua('/srv/scratch/avanti/ExPecto/resources/deepsea.beluga.2002.cpu')\n",
    "model.evaluate()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def encodeSeqs(seqs, inputsize=2000):\n",
    "    \"\"\"Convert sequences to 0-1 encoding and truncate to the input size.\n",
    "    The output concatenates the forward and reverse complement sequence\n",
    "    encodings.\n",
    "    Args:\n",
    "        seqs: list of sequences (e.g. produced by fetchSeqs)\n",
    "        inputsize: the number of basepairs to encode in the output\n",
    "    Returns:\n",
    "        numpy array of dimension: (2 x number of sequence) x 4 x inputsize\n",
    "    2 x number of sequence because of the concatenation of forward and reverse\n",
    "    complement sequences.\n",
    "    \"\"\"\n",
    "    seqsnp = np.zeros((len(seqs), 4, inputsize), np.bool_)\n",
    "\n",
    "    mydict = {'A': np.asarray([1, 0, 0, 0]), 'G': np.asarray([0, 1, 0, 0]),\n",
    "            'C': np.asarray([0, 0, 1, 0]), 'T': np.asarray([0, 0, 0, 1]),\n",
    "            'N': np.asarray([0, 0, 0, 0]), 'H': np.asarray([0, 0, 0, 0]),\n",
    "            'a': np.asarray([1, 0, 0, 0]), 'g': np.asarray([0, 1, 0, 0]),\n",
    "            'c': np.asarray([0, 0, 1, 0]), 't': np.asarray([0, 0, 0, 1]),\n",
    "            'n': np.asarray([0, 0, 0, 0]), '-': np.asarray([0, 0, 0, 0])}\n",
    "\n",
    "    n = 0\n",
    "    for line in seqs:\n",
    "        cline = line[int(math.floor(((len(line) - inputsize) / 2.0))):int(math.floor(len(line) - (len(line) - inputsize) / 2.0))]\n",
    "        for i, c in enumerate(cline):\n",
    "            seqsnp[n, :, i] = mydict[c]\n",
    "        n = n + 1\n",
    "\n",
    "    # get the complementary sequences\n",
    "    #dataflip = seqsnp[:, ::-1, ::-1]\n",
    "    #seqsnp = np.concatenate([seqsnp, dataflip], axis=0)\n",
    "    return seqsnp.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shifts:\n",
      " [-19900 -19700 -19500 -19300 -19100 -18900 -18700 -18500 -18300 -18100\n",
      " -17900 -17700 -17500 -17300 -17100 -16900 -16700 -16500 -16300 -16100\n",
      " -15900 -15700 -15500 -15300 -15100 -14900 -14700 -14500 -14300 -14100\n",
      " -13900 -13700 -13500 -13300 -13100 -12900 -12700 -12500 -12300 -12100\n",
      " -11900 -11700 -11500 -11300 -11100 -10900 -10700 -10500 -10300 -10100\n",
      "  -9900  -9700  -9500  -9300  -9100  -8900  -8700  -8500  -8300  -8100\n",
      "  -7900  -7700  -7500  -7300  -7100  -6900  -6700  -6500  -6300  -6100\n",
      "  -5900  -5700  -5500  -5300  -5100  -4900  -4700  -4500  -4300  -4100\n",
      "  -3900  -3700  -3500  -3300  -3100  -2900  -2700  -2500  -2300  -2100\n",
      "  -1900  -1700  -1500  -1300  -1100   -900   -700   -500   -300   -100\n",
      "    100    300    500    700    900   1100   1300   1500   1700   1900\n",
      "   2100   2300   2500   2700   2900   3100   3300   3500   3700   3900\n",
      "   4100   4300   4500   4700   4900   5100   5300   5500   5700   5900\n",
      "   6100   6300   6500   6700   6900   7100   7300   7500   7700   7900\n",
      "   8100   8300   8500   8700   8900   9100   9300   9500   9700   9900\n",
      "  10100  10300  10500  10700  10900  11100  11300  11500  11700  11900\n",
      "  12100  12300  12500  12700  12900  13100  13300  13500  13700  13900\n",
      "  14100  14300  14500  14700  14900  15100  15300  15500  15700  15900\n",
      "  16100  16300  16500  16700  16900  17100  17300  17500  17700  17900\n",
      "  18100  18300  18500  18700  18900  19100  19300  19500  19700  19900]\n"
     ]
    }
   ],
   "source": [
    "import pyfasta\n",
    "\n",
    "hg19_path = \"/mnt/data/annotations/by_organism/human/hg19.GRCh37/hg19.genome.fa\"\n",
    "genome = pyfasta.Fasta(hg19_path)\n",
    "\n",
    "windowsize = 2000\n",
    "predictions_fwdonly = []\n",
    "predictions_withrc = []\n",
    "maxshift=20000\n",
    "shifts = np.array(list(range(-20000,20000,200)))+100\n",
    "print(\"shifts:\\n\",shifts)\n",
    "assert len(shifts)==200\n",
    "for gene,chrom,tss,strand in gene_chrom_tss_strand[:3]:\n",
    "    seqs_to_predict = []\n",
    "    for shift in shifts:\n",
    "        seq = genome.sequence({'chr': chrom, 'start': tss + shift*strand -\n",
    "                               int(0.5*windowsize - 1), 'stop': tss + shift*strand + int(0.5*windowsize)})\n",
    "        seqs_to_predict.append(seq)\n",
    "\n",
    "    seqsnp = encodeSeqs(seqs_to_predict)\n",
    "\n",
    "    model_input = torch.from_numpy(np.array(seqsnp)).unsqueeze(3)\n",
    "    rc_model_input = torch.from_numpy(np.array(seqsnp[:,::-1,::-1])).unsqueeze(3)\n",
    "    model_input = model_input.cuda()\n",
    "    rc_model_input = rc_model_input.cuda()\n",
    "    prediction = model.forward(model_input).cpu().numpy().copy()\n",
    "    rc_prediction = model.forward(rc_model_input).cpu().numpy().copy()\n",
    "    predictions_fwdonly.append(prediction)\n",
    "    predictions_withrc.append(0.5*(prediction+rc_prediction))\n",
    "\n",
    "predictions_fwdonly=np.array(predictions_fwdonly)\n",
    "predictions_withrc=np.array(predictions_withrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_weight_shifts = shifts\n",
    "pos_weights = np.vstack([\n",
    "        np.exp(-0.01*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts <= 0),\n",
    "        np.exp(-0.02*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts <= 0),\n",
    "        np.exp(-0.05*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts <= 0),\n",
    "        np.exp(-0.1*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts <= 0),\n",
    "        np.exp(-0.2*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts <= 0),\n",
    "        np.exp(-0.01*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts >= 0),\n",
    "        np.exp(-0.02*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts >= 0),\n",
    "        np.exp(-0.05*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts >= 0),\n",
    "        np.exp(-0.1*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts >= 0),\n",
    "        np.exp(-0.2*np.abs(pos_weight_shifts)/200)*(pos_weight_shifts >= 0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstructed_expecto_fwdonly = np.sum(pos_weights[None,:,:,None]*predictions_fwdonly[:,None,:,:],axis=2)\n",
    "reconstructed_expecto_withrc = np.sum(pos_weights[None,:,:,None]*predictions_withrc[:,None,:,:],axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.35015494 1.03121655 0.69268325 0.58555646 0.53042333 0.52554336\n",
      " 0.47175934 0.39041655 0.32987617 0.26208855]\n",
      "[1.35120312 1.02925651 0.68654915 0.57727376 0.52144186 0.57660481\n",
      " 0.51948136 0.42981128 0.35950798 0.28117582]\n"
     ]
    }
   ],
   "source": [
    "print(reconstructed_expecto_fwdonly[0,:,0])\n",
    "print(reconstructed_expecto_withrc[0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.22746855, 0.91435068, 0.5421533 , 0.41338246, 0.36833563,\n",
       "       0.67654061, 0.62398969, 0.54676042, 0.49142184, 0.43067582])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expecto_features[0].reshape(10,2002)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpearmanrResult(correlation=0.9900563526481763, pvalue=0.0)\n",
      "SpearmanrResult(correlation=0.9918444705437338, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import spearmanr\n",
    "print(spearmanr(reconstructed_expecto_fwdonly.flatten(), expecto_features[:len(reconstructed_expecto)].flatten()))\n",
    "print(spearmanr(reconstructed_expecto_withrc.flatten(), expecto_features[:len(reconstructed_expecto)].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
